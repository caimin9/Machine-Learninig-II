#######################################################
######## Regression or Classification?
#######################################################
Regression:
This is a regression problem because the target variable is continuous, representing a numeric quantity that can take any value within a range. 
The objective is to predict this numeric value based on the input features.
Classification:
This is a classification problem because the target variable consists of discrete categories or classes. 
The objective is to assign each observation to the correct category based on the input features.



#######################################################
######## Correlation Filter
#######################################################








#######################################################
######## Fitting RF, GBM, Tree, Pruned Tree
#######################################################
######### TREE #########
my_tree = tree(High~.,data = CS, subset = index) 
######### BAGGED TREE #########
bag.tree = randomForest(High~.,data = CS,subset = index, mtry = P) #BAGGED TREE (SINCE MTRY = #OF PREDICTORS)
######### PRUNED TREE #########
prune_tree = cv.tree(my_tree, FUN = prune.misclass) 
opt_pt = prune.misclass(my_tree, best = opt_size)
######### RF #########
rf.out = randomForest(High~., CS, ntree=5000) #A random forest fitted usng 5000 trees
######### GBM #########
gb.out = gbm(High~., data=CS, 
             distribution="bernoulli", # use "gaussian" instead for regression
             n.trees=5000, # size of the ensemble
             interaction.depth=1) # depth of the trees, 1 = stumps
### USing caret
my_gbm = train(High~., data = dat.train, method = 'gbm')
gbm_fitted = predict(my_gbm) #To get fitted values
#######################################################
######## Neural Network Scaling
#######################################################

#minmax scaling
minmax_scale = function (x) {
return (( x - min (x) ) / ( max (x) - min( x))) }






#######################################################
######## Recode Levels
#######################################################
# Makes it binary (1,0) --> Regression Problems & GBM with Bernoulli distribution
CS$High = as.numeric(CS$High == 'Yes')

#Makes it into factors (Yes, No) --> Classification Problems
CS$High = as.factor(CS$High)


#######################################################
######## MOST IMPORTANT VARIABLES
#######################################################
# If needed, top 5 variables by MeanDecreaseGini:
imp.rf <- importance(rf.fit, type = 2)
head(sort(imp.rf[,1], decreasing = TRUE), 5)


#######################################################
######## How tree methods choose their nodes
#######################################################
The algorithm searches over every possible feature and every possible cut point and picks the one that maximises the drop in impurity 
or equivalently minimises the drop in RSS

For bagging and trees both look at all features whereas random forests look at a subset of features
- Regression problems we look at RSS
- Classification we look at maximising the gini coefficinet index


These are greedy algorithms though so
- Local Optima: We may miss a better global structure by committing too early
- Variable Selection Bias: RSS & Gini tends to favour variables with many possible split points
- Axis-aligned Splits only: Interactions between features can be missed

Could use entropy as a loss function to reduce bias
Could also use gradient boosting to cope with this problem



#######################################################
######## KNN ROC ANALYSIS ALTERATION
#######################################################




#######################################################
######## SVM SLACK VARIABLES
#######################################################
